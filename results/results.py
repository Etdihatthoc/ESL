import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# 1. Äá»c CSV
df = pd.read_csv("/media/gpus/Data/AES/ESL-Grading/results/test_predictions.csv")

# 2. LÃ m trÃ²n cá»™t Predict Score Ä‘á»ƒ cÃ³ Ä‘iá»ƒm rá»i ráº¡c (0.5 increments)
df["Predict Score"] = np.round(df["Predict Score"] * 2) / 2
df["Predict Score Rounded"] = np.round(df["Predict Score"] * 2) / 2
df["Absolute Error"] = np.abs(df["GroundTruth"] - df["Predict Score Rounded"])
# 3. TÃ­nh MSE vÃ  MAE tá»•ng thá»ƒ
mse_overall = mean_squared_error(df["GroundTruth"], df["Predict Score Rounded"])
mae_overall = mean_absolute_error(df["GroundTruth"], df["Predict Score Rounded"])

print("="*50)
print("METRICS EVALUATION REPORT")
print("="*50)
print(f"ðŸ“Š OVERALL METRICS:")
print(f"MSE: {mse_overall:.4f}")
print(f"MAE: {mae_overall:.4f}")
print(f"Total samples: {len(df)}")

# 4. TÃ­nh metrics theo range Ä‘iá»ƒm
print(f"\nðŸ“Š METRICS BY SCORE RANGES:")

# Range [3-7.5]
range_3_75 = df[(df["GroundTruth"] >= 3) & (df["GroundTruth"] <= 7.5)]
if len(range_3_75) > 0:
    mse_3_75 = mean_squared_error(range_3_75["GroundTruth"], range_3_75["Predict Score Rounded"])
    mae_3_75 = mean_absolute_error(range_3_75["GroundTruth"], range_3_75["Predict Score Rounded"])
    print(f"Range [3.0-7.5]: MSE={mse_3_75:.4f}, MAE={mae_3_75:.4f}, Count={len(range_3_75)}")

# Range [8-10]
range_8_10 = df[(df["GroundTruth"] >= 8) & (df["GroundTruth"] <= 10)]
if len(range_8_10) > 0:
    mse_8_10 = mean_squared_error(range_8_10["GroundTruth"], range_8_10["Predict Score Rounded"])
    mae_8_10 = mean_absolute_error(range_8_10["GroundTruth"], range_8_10["Predict Score Rounded"])
    print(f"Range [8.0-10.0]: MSE={mse_8_10:.4f}, MAE={mae_8_10:.4f}, Count={len(range_8_10)}")

# 5. TÃ­nh metrics theo tá»«ng Ä‘iá»ƒm cá»¥ thá»ƒ tá»« 3 Ä‘áº¿n 10
print(f"\nðŸ“Š METRICS BY INDIVIDUAL SCORES:")
all_scores = [3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0, 9.5, 10.0]
for score in all_scores:
    score_data = df[df["GroundTruth"] == score]
    if len(score_data) > 0:
        mse_score = mean_squared_error(score_data["GroundTruth"], score_data["Predict Score Rounded"])
        mae_score = mean_absolute_error(score_data["GroundTruth"], score_data["Predict Score Rounded"])
        print(f"Score {score}: MSE={mse_score:.4f}, MAE={mae_score:.4f}, Count={len(score_data)}")

# 6. Top 10 dá»± Ä‘oÃ¡n lá»‡ch xa nháº¥t
print(f"\nðŸ“Š TOP 10 WORST PREDICTIONS:")
worst_predictions = df.nlargest(10, 'Absolute Error')[['GroundTruth', 'Predict Score', 'Absolute Error']]
print(worst_predictions.round(3).to_string(index=False))

# 7. Váº½ vÃ  lÆ°u Confusion Matrix
plt.figure(figsize=(10, 8))
# Táº¡o confusion matrix cho range 3-10
score_range = np.arange(3, 10.5, 0.5)
confusion_data = np.zeros((len(score_range), len(score_range)))

for i, gt_score in enumerate(score_range):
    for j, pred_score in enumerate(score_range):
        count = len(df[(df["GroundTruth"] == gt_score) & (df["Predict Score Rounded"] == pred_score)])
        confusion_data[i, j] = count

sns.heatmap(confusion_data, annot=True, fmt='g', cmap='Blues', 
            xticklabels=[f'{x:.1f}' for x in score_range], 
            yticklabels=[f'{x:.1f}' for x in score_range])
plt.title('Confusion Matrix (Score Range 3-10)', fontweight='bold', fontsize=14)
plt.xlabel('Predicted Score', fontsize=12)
plt.ylabel('Ground Truth', fontsize=12)
plt.tight_layout()
plt.savefig('/media/gpus/Data/AES/ESL-Grading/results/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()

# 8. Váº½ biá»ƒu Ä‘á»“ chÃªnh lá»‡ch theo tá»«ng má»©c Ä‘iá»ƒm
plt.figure(figsize=(12, 6))

# TÃ­nh error cho tá»«ng Ä‘iá»ƒm
score_points = []
positive_errors = []  # Dá»± Ä‘oÃ¡n > tháº­t
negative_errors = []  # Dá»± Ä‘oÃ¡n < tháº­t

for score in all_scores:
    score_data = df[df["GroundTruth"] == score]
    if len(score_data) > 0:
        errors = score_data["Predict Score Rounded"] - score_data["GroundTruth"]
        pos_error = errors[errors > 0].mean() if len(errors[errors > 0]) > 0 else 0
        neg_error = errors[errors < 0].mean() if len(errors[errors < 0]) > 0 else 0
        
        score_points.append(score)
        positive_errors.append(pos_error)
        negative_errors.append(neg_error)

# Váº½ Ä‘Æ°á»ng tháº³ng cho cÃ¡c Ä‘iá»ƒm 3-10
plt.plot(score_points, [0] * len(score_points), 'k-', linewidth=2, label='Perfect Prediction')

# Váº½ cá»™t thá»ƒ hiá»‡n chÃªnh lá»‡ch
width = 0.1
for i, score in enumerate(score_points):
    if positive_errors[i] > 0:
        plt.bar(score, positive_errors[i], width, color='red', alpha=0.7, label='Overestimation' if i == 0 else "")
    if negative_errors[i] < 0:
        plt.bar(score, negative_errors[i], width, color='blue', alpha=0.7, label='Underestimation' if i == 0 else "")

plt.xlabel('Ground Truth Score', fontsize=12)
plt.ylabel('Mean Prediction Error', fontsize=12)
plt.title('Prediction Error by Score Level', fontweight='bold', fontsize=14)
plt.grid(True, alpha=0.3)
plt.legend()
plt.xticks(score_points)
plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)

# ThÃªm text annotation cho cÃ¡c giÃ¡ trá»‹
for i, score in enumerate(score_points):
    if positive_errors[i] > 0:
        plt.text(score, positive_errors[i] + 0.02, f'{positive_errors[i]:.3f}', 
                ha='center', va='bottom', fontsize=9)
    if negative_errors[i] < 0:
        plt.text(score, negative_errors[i] - 0.02, f'{negative_errors[i]:.3f}', 
                ha='center', va='top', fontsize=9)

plt.tight_layout()
plt.savefig('/media/gpus/Data/AES/ESL-Grading/results/error_by_score.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nâœ… Saved plots to:")
print(f"- /media/gpus/Data/AES/ESL-Grading/results/confusion_matrix.png")
print(f"- /media/gpus/Data/AES/ESL-Grading/results/error_by_score.png")

print("\n" + "="*50)
print("EVALUATION COMPLETED")
print("="*50)