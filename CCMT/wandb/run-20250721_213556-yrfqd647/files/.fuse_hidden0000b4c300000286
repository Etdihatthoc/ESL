2025-07-21 21:36:09,366 - __main__ - INFO - Weights & Biases initialized successfully
2025-07-21 21:36:09,429 - __main__ - INFO - Using device: cuda
2025-07-21 21:36:09,462 - __main__ - INFO - CUDA device: NVIDIA GeForce RTX 2080 Ti
2025-07-21 21:36:09,463 - __main__ - INFO - CUDA memory: 11.5 GB
2025-07-21 21:36:09,463 - __main__ - INFO - Creating CCMT model...
Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-07-21 21:37:32,243 - __main__ - INFO - Model created with 365,554,602 total parameters
2025-07-21 21:37:32,244 - __main__ - INFO - Trainable parameters: 365,554,602
2025-07-21 21:37:32,244 - __main__ - INFO - Creating text processor...
Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
ASR model openai/whisper-base loaded successfully on CPU
Translation model Helsinki-NLP/opus-mt-en-vi loaded successfully on CPU
2025-07-21 21:37:35,975 - __main__ - INFO - Text processor created successfully
2025-07-21 21:37:35,976 - __main__ - INFO - Creating trainer...
2025-07-21 21:37:36,799 - __main__ - INFO - Preparing datasets...
2025-07-21 21:37:36,893 - __main__ - INFO - Data sizes - Train: 5879, Val: 840, Test: 1680
English tokenizer max length: 510
Vietnamese tokenizer max length: 256
Rows before cleaning: 5879
Rows after low-content filtering: 5811
After score filtering: 5600 samples
Score distribution: 3.0     396
3.5     284
4.0     916
4.5     317
5.0     946
5.5     290
6.0     951
6.5     265
7.0     599
7.5     145
8.0     320
8.5     107
9.0      54
9.5       7
10.0      3
Name: grammar, dtype: int64
Rows after cleaning: 5600
English tokenizer max length: 510
Vietnamese tokenizer max length: 256
Rows before cleaning: 840
Rows after low-content filtering: 834
After score filtering: 797 samples
Score distribution: 3.0     54
3.5     33
4.0    131
4.5     49
5.0    141
5.5     40
6.0    146
6.5     39
7.0     76
7.5     22
8.0     41
8.5     17
9.0      8
Name: grammar, dtype: int64
Rows after cleaning: 797
English tokenizer max length: 510
Vietnamese tokenizer max length: 256
Rows before cleaning: 1680
Rows after low-content filtering: 1666
After score filtering: 1592 samples
Score distribution: 3.0    103
3.5     77
4.0    254
4.5     77
5.0    308
5.5     87
6.0    258
6.5     78
7.0    168
7.5     40
8.0     95
8.5     32
9.0     13
9.5      2
Name: grammar, dtype: int64
Rows after cleaning: 1592
2025-07-21 21:38:28,091 - __main__ - INFO - Dataset sizes after filtering - Train: 5600, Val: 797, Test: 1592
Score distribution before sampling:
  Score 3.0: 396 samples
  Score 3.5: 284 samples
  Score 4.0: 916 samples
  Score 4.5: 317 samples
  Score 5.0: 946 samples
  Score 5.5: 290 samples
  Score 6.0: 951 samples
  Score 6.5: 265 samples
  Score 7.0: 599 samples
  Score 7.5: 145 samples
  Score 8.0: 320 samples
  Score 8.5: 107 samples
  Score 9.0: 54 samples
  Score 9.5: 7 samples
  Score 10.0: 3 samples
Effective sampling distribution (alpha=0.5):
  Score 3.0: 0.0777
  Score 3.5: 0.0658
  Score 4.0: 0.1181
  Score 4.5: 0.0695
  Score 5.0: 0.1201
  Score 5.5: 0.0665
  Score 6.0: 0.1204
  Score 6.5: 0.0635
  Score 7.0: 0.0955
  Score 7.5: 0.0470
  Score 8.0: 0.0698
  Score 8.5: 0.0404
  Score 9.0: 0.0287
  Score 9.5: 0.0103
  Score 10.0: 0.0068
2025-07-21 21:38:28,255 - __main__ - INFO - Class weights calculated: tensor([2.7215, 2.7215, 2.7215, 2.7215, 2.7215, 2.7215, 0.1500, 0.1744, 0.1037,
        0.1659, 0.1023, 0.1727, 0.1021, 0.1800, 0.1247, 0.2380, 0.1652, 0.2744,
        0.3789, 0.9711, 1.3676], device='cuda:0')
Parameter groups: base=169, encoder=609, special=0
2025-07-21 21:38:28,492 - __main__ - INFO - Training setup complete. Total steps: 140000
2025-07-21 21:38:28,492 - __main__ - INFO - Starting training...
2025-07-21 21:38:28,492 - __main__ - INFO - Starting training...
Selective freezing applied to 690 tokens out of 30522
Selective freezing applied to 784 tokens out of 64001
2025-07-21 21:38:31,231 - __main__ - INFO - Selective freezing applied to embedding layers











































Training Epoch 1:   1%|‚ñç                                              | 48/5600 [03:29<6:44:18,  4.37s/it, Loss=1.7925, MAE=0.8564, KL=1.9102, MSE=0.7335]
Traceback (most recent call last):
  File "train_ccmt.py", line 326, in <module>
    sys.exit(main())
  File "train_ccmt.py", line 290, in main
    trainer.train()
  File "/media/gpus/Data/AES/ESL-Grading/CCMT/models/trainer.py", line 449, in train
    train_metrics = self.train_epoch(epoch)
  File "/media/gpus/Data/AES/ESL-Grading/CCMT/models/trainer.py", line 287, in train_epoch
    for batch in pbar:
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1327, in _next_data
    idx, data = self._get_data()
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1283, in _get_data
    success, data = self._try_get_data()
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1131, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
Exception ignored in: <module 'threading' from '/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/threading.py'>
Traceback (most recent call last):
  File "/mnt/disk1/anaconda3/envs/dinhson/lib/python3.8/threading.py", line 1388, in _shutdown
    lock.acquire()
KeyboardInterrupt: